{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/dask_horizontal.svg\" align=\"right\" width=\"30%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 包：半结构化数据的并行列表："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask-bag擅长处理可以表示为任意输入序列的数据。我们将其称为 \"混乱 \"数据，因为它可以包含复杂的嵌套结构、缺失的字段、数据类型的混合物等。这种*函数式的编程风格很适合标准的Python迭代，比如可以在`itertools`模块中找到。\n",
    "\n",
    "在数据处理流水线的初期，当大量的原始数据被首次消耗时，经常会遇到混乱的数据。初始数据集可能是JSON、CSV、XML或任何其他不执行严格结构和数据类型的格式。\n",
    "出于这个原因，最初的数据群发和处理通常使用Python的`list`s、`dict`s和`set`s来完成。\n",
    "\n",
    "这些核心数据结构是为通用存储和处理而优化的。 用迭代器/生成器表达式或像`itertools`或[`toolz`](https://toolz.readthedocs.io/en/latest/)这样的库添加流式计算，可以让我们在小空间里处理大量的数据。 如果我们将其与并行处理结合起来，那么我们可以搅动相当数量的数据。\n",
    "\n",
    "Dask.bag是一个高级的Dask集合，用于自动化这种形式的常见工作负载。 简而言之\n",
    "\n",
    "    dask.bag = map、filter、toolz +并行执行\n",
    "\n",
    "**相关文档**：\n",
    "\n",
    "* [Bag documentation](https://docs.dask.org/en/latest/bag.html)\n",
    "* [Bag screencast](https://youtu.be/-qIiJ1XtSv0)\n",
    "* [Bag API](https://docs.dask.org/en/latest/bag-api.html)\n",
    "* [Bag examples](https://examples.dask.org/bag.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run prep.py -d accounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 设置"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同样，我们将使用分布式调度器。调度器将在[后面](05_distributed.ipynb)进行深入解释。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以从Python序列、从文件、从S3上的数据等创建一个`Bag`。\n",
    "我们演示使用`.take()`来显示数据的元素。(执行`.take(1)`的结果是一个有一个元素的元组)\n",
    "\n",
    "请注意，数据被分割成块，每个块有很多项。在第一个例子中，两个分区各包含5个元素，在下面的两个例子中，每个文件被分割成一个或多个字节块。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个元素都是一个整数\n",
    "import dask.bag as db\n",
    "b = db.from_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10], npartitions=2)\n",
    "b.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 每个元素是一个文本文件，其中每行是一个JSON对象。\n",
    "# 注意，压缩是自动处理的\n",
    "import os\n",
    "b = db.read_text(os.path.join('data', 'accounts.*.json.gz'))\n",
    "b.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 编辑sources.py来配置源位置\n",
    "import sources\n",
    "sources.bag_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Requires `s3fs` library\n",
    "# each partition is a remote CSV text file\n",
    "b = db.read_text(sources.bag_url,\n",
    "                 storage_options={'anon': True})\n",
    "b.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 操作"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Bag`对象拥有Python标准库、`toolz`或`pyspark`等项目中的标准功能API，包括`map`、`filter`、`groupby`等。\n",
    "\n",
    "对`Bag`对象的操作会创建新的袋子。 调用`.compute()`方法来触发执行，就像我们对`Delayed`对象的操作一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_even(n):\n",
    "    return n % 2 == 0\n",
    "\n",
    "b = db.from_sequence([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])\n",
    "c = b.filter(is_even).map(lambda x: x ** 2)\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 阻断形式：等待完成(在这种情况下是非常快的)\n",
    "c.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 例子: 账户JSON数据"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经在你的数据目录中创建了一个gzipped JSON数据的假数据集。 这就像我们稍后看到的 \"DataFrame \"示例中使用的例子一样，只是它将每个单独的 \"id \"的所有内容捆绑成了一条记录。 这类似于你可能从文档存储数据库或Web API中收集的数据。\n",
    "\n",
    "每一行都是一个JSON编码的字典，其键如下\n",
    "\n",
    "* id：客户的唯一标识符\n",
    "* 名称：客户名称\n",
    "* 交易：`transaction-id'、`amount'对的清单，该文件中客户的每一笔交易都有一个。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = os.path.join('data', 'accounts.*.json.gz')\n",
    "lines = db.read_text(filename)\n",
    "lines.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们的数据以文本行的形式从文件中出来。注意，文件解压是自动发生的。我们可以通过将`json.loads`函数映射到我们的袋子上，让这个数据看起来更合理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "js = lines.map(json.loads)\n",
    "# take: inspect first few elements\n",
    "js.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 基本查询"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一旦我们将JSON数据解析成合适的Python对象(`dict`s，`list`s等)，我们就可以通过创建小的Python函数在我们的数据上运行来执行更有趣的查询。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter: keep only some elements of the sequence\n",
    "js.filter(lambda record: record['name'] == 'Alice').take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_transactions(d):\n",
    "    return {'name': d['name'], 'count': len(d['transactions'])}\n",
    "\n",
    "# map: apply a function to each element\n",
    "(js.filter(lambda record: record['name'] == 'Alice')\n",
    "   .map(count_transactions)\n",
    "   .take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pluck: select a field, as from a dictionary, element[field]\n",
    "(js.filter(lambda record: record['name'] == 'Alice')\n",
    "   .map(count_transactions)\n",
    "   .pluck('count')\n",
    "   .take(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average number of transactions for all of the Alice entries\n",
    "(js.filter(lambda record: record['name'] == 'Alice')\n",
    "   .map(count_transactions)\n",
    "   .pluck('count')\n",
    "   .mean()\n",
    "   .compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use `flatten` to de-nest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the example below we see the use of `.flatten()` to flatten results.  We compute the average amount for all transactions for all Alices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(js.filter(lambda record: record['name'] == 'Alice')\n",
    "   .pluck('transactions')\n",
    "   .take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(js.filter(lambda record: record['name'] == 'Alice')\n",
    "   .pluck('transactions')\n",
    "   .flatten()\n",
    "   .take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(js.filter(lambda record: record['name'] == 'Alice')\n",
    "   .pluck('transactions')\n",
    "   .flatten()\n",
    "   .pluck('amount')\n",
    "   .take(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(js.filter(lambda record: record['name'] == 'Alice')\n",
    "   .pluck('transactions')\n",
    "   .flatten()\n",
    "   .pluck('amount')\n",
    "   .mean()\n",
    "   .compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby和Foldby"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们经常想通过一些函数或键对数据进行分组。 我们可以使用`.groupby`方法来实现这一目标，该方法简单明了，但会强制对数据进行完全洗牌（成本很高），也可以使用更难使用但更快的`.foldby`方法，该方法将groupby和reduction结合在一起进行流式处理。\n",
    "\n",
    "* `groupby`。 洗牌数据，使所有键值相同的项都在同一个键值对中。\n",
    "* `foldby`。 遍历数据，按键累积结果。\n",
    "\n",
    "*注意：完整的groupby特别糟糕。在实际工作中，你最好使用 \"foldby\"，或尽可能改用 \"DataFrame\"。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `groupby`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Groupby收集集合中的项目，使所有在某个函数下具有相同值的项目被收集在一起，成为一个键值对。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = db.from_sequence(['Alice', 'Bob', 'Charlie', 'Dan', 'Edith', 'Frank'])\n",
    "b.groupby(len).compute()  # 根据名字的长度分组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = db.from_sequence(list(range(10)))\n",
    "b.groupby(lambda x: x % 2).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.groupby(lambda x: x % 2).starmap(lambda k, v: (k, max(v))).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `foldby`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Foldby一开始可能很奇怪。 它与其他库的以下函数类似。\n",
    "\n",
    "* [`toolz.reduceby`](http://toolz.readthedocs.io/en/latest/streaming-analytics.html#streaming-split-apply-combine)\n",
    "* [`pyspark.RDD.combinedByKey'](http://abshinn.github.io/python/apache-spark/2014/10/11/using-combinebykey-in-apache-spark/)\n",
    "\n",
    "当使用 \"foldby \"时，您提供的是\n",
    "\n",
    "1.  对要素进行分组的关键功能\n",
    "2.  一个二进制运算符，比如你会传递给`reduce`，你用来对每组进行还原。\n",
    "3.  组合二元运算符，可以将数据集不同部分的两次`reduce`调用的结果组合起来。\n",
    "\n",
    "你的还原必须是关联性的，它将在你的数据集的每个分区中并行发生。 它将在你的数据集的每个分区中并行发生。 然后，所有这些中间结果将由`combine`二进制操作符合并。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b.foldby(lambda x: x % 2, binop=max, combine=max).compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 带账户数据的示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们发现同名的人数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# 警告，这个需要一段时间... ...\n",
    "result = js.groupby(lambda item: item['name']).starmap(lambda k, v: (k, len(v))).compute()\n",
    "print(sorted(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# This one is comparatively fast and produces the same result.\n",
    "from operator import add\n",
    "def incr(tot, _):\n",
    "    return tot + 1\n",
    "\n",
    "result = js.foldby(key='name', \n",
    "                   binop=incr, \n",
    "                   initial=0, \n",
    "                   combine=add, \n",
    "                   combine_initial=0).compute()\n",
    "print(sorted(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：计算每个名字的总金额"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们想把`name`键进行分组，然后把每个名字的金额加起来。\n",
    "\n",
    "步骤\n",
    "\n",
    "1.  创建一个小函数，给定一个像{'name': 'Alice', 'transactions': [{'金额': 1, 'id': 123}, {'金额': 2, 'id': 456}]}产生金额的总和，例如：`3`。\n",
    "\n",
    "2.  稍微改变上面`foldby`例子的二进制运算符，使二进制运算符不计算条目数，而是累计金额之和。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "出于同样的原因，Pandas通常比纯Python快，`dask.dataframe`可以比`dask.bag`快。 后面我们会更多地使用DataFrames，但从Bag的角度来看，它经常是数据摄取中 \"混乱 \"部分的终点--一旦数据可以做成数据框架，那么复杂的拆分-应用-合并逻辑将变得更加直接和高效。\n",
    "\n",
    "你可以用`to_dataframe`方法将一个简单的元组或平面字典结构的袋子转化为`dask.dataframe`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = js.to_dataframe()\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，这看起来就像一个定义良好的DataFrame，我们可以有效地对它进行类似Pandas的计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用Dask DataFrame，我们事先计算同名同姓的人数需要多长时间？ 事实证明，`dask.dataframe.groupby()`比`dask.bag.groupby()`强了不止一个数量级；但是，它仍然无法与`dask.bag.foldby()`相提并论。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df1.groupby('name').id.count().compute().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 非正常化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这种DataFrame格式不太理想，因为`transactions`列充满了嵌套的数据，所以Pandas必须恢复到`object`dtype，这在Pandas中是相当慢的。 理想的情况是，我们只有在将数据扁平化，使每条记录都是一个单一的 \"int\"、\"string\"、\"float \"等之后，才想转换为数据框架。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denormalize(record):\n",
    "    # returns a list for each person, one item per transaction\n",
    "    return [{'id': record['id'], \n",
    "             'name': record['name'], \n",
    "             'amount': transaction['amount'], \n",
    "             'transaction-id': transaction['transaction-id']}\n",
    "            for transaction in record['transactions']]\n",
    "\n",
    "transactions = js.map(denormalize).flatten()\n",
    "transactions.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = transactions.to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# number of transactions per name\n",
    "# note that the time here includes the data load and ingestion\n",
    "df.groupby('name')['transaction-id'].count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 局限"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "袋子提供了非常通用的计算(任何Python函数。)这种通用性是指\n",
    "是有成本的。 袋子有以下已知的限制\n",
    "\n",
    "1.  袋式运算往往比数组/数据框计算慢，在这种情况下，袋式运算的速度会更快。\n",
    "    就像Python比NumPy/Pandas慢一样。\n",
    "2.  ``Bag.groupby``很慢。 如果可能的话，你应该尝试使用``Bag.foldby``。\n",
    "    使用``Bag.foldby``需要更多的思考。更好的做法是，考虑创建\n",
    "    归一化数据框。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 更多信息，参考：\n",
    "\n",
    "* [Bag documentation](https://docs.dask.org/en/latest/bag.html)\n",
    "* [Bag screencast](https://youtu.be/-qIiJ1XtSv0)\n",
    "* [Bag API](https://docs.dask.org/en/latest/bag-api.html)\n",
    "* [Bag examples](https://examples.dask.org/bag.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 关闭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}