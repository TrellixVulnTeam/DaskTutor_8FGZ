{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://dask.readthedocs.io/en/latest/_images/dask_horizontal.svg\"\n",
    "     align=\"right\"\n",
    "     width=\"30%\"\n",
    "     alt=\"Dask logo\\\">\n",
    "\n",
    "\n",
    "# Dask数据框\n",
    "\n",
    "我们使用`dask.delayed`在CSV文件的目录上建立一个并行的数据帧计算，从而结束了第一章。 在本节中，我们使用`dask.dataframe`来自动构建类似的计算，用于常见的表格计算。 Dask数据框看起来和感觉都像Pandas数据框，但它们运行在与`dask.delayed`相同的基础设施上。\n",
    "\n",
    "在这个笔记本中，我们使用了和以前一样的航空公司数据，但现在我们不写for-loops，而是让`dask.dataframe`为我们构造计算。 `dask.dataframe.read_csv`函数可以接受一个像`\"data/nycflights/*.csv\"`这样的globstring，并一次对我们所有的数据进行并行计算。\n",
    "\n",
    "## 何时使用`dask.dataframe`？\n",
    "\n",
    "Pandas对于能在内存中处理的表格数据集是非常优秀的工具。当你要分析的数据集大于你的机器内存时，Dask就变得很有用。我们正在使用的演示数据集只有大约200MB，所以你可以在合理的时间内下载它，但`dask.dataframe`将扩展到比内存大得多的数据集。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/pandas_logo.png\" align=\"right\" width=\"28%\">\n",
    "\n",
    "`dask.dataframe`模块实现了一个阻塞的并行`DataFrame`对象，它模仿了Pandas`DataFrame`API的一个子集。一个Dask`DataFrame`是由许多内存中的pandas`DataFrame`组成，沿着索引分开。对Dask`DataFrame`的一个操作会触发对组成的pandas`DataFrame`的许多pandas操作，这种方式是注意潜在的并行性和内存限制。\n",
    "\n",
    "**相关文档**\n",
    "\n",
    "* [DataFrame documentation](https://docs.dask.org/en/latest/dataframe.html)\n",
    "* [DataFrame screencast](https://youtu.be/AT2XtFehFSQ)\n",
    "* [DataFrame API](https://docs.dask.org/en/latest/dataframe-api.html)\n",
    "* [DataFrame examples](https://examples.dask.org/dataframe.html)\n",
    "* [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/)\n",
    "\n",
    "**主要收获**\n",
    "\n",
    "1.  Dask DataFrame应该是Pandas用户所熟悉的了\n",
    "2.  数据框的分区对高效执行很重要。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run prep.py -d flights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask.distributed import Client\n",
    "\n",
    "client = Client(n_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "创建了人工数据。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from prep import accounts_csvs\n",
    "accounts_csvs()\n",
    "\n",
    "import os\n",
    "import dask\n",
    "filename = os.path.join('data', 'accounts.*.csv')\n",
    "filename"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文件名包含一个 glob 模式 `*`，因此路径中与该模式匹配的所有文件都将被读入同一个 Dask DataFrame。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "df = dd.read_csv(filename)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载计算行数\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里发生了什么？\n",
    "- Dask调查了输入路径，发现有三个匹配的文件。\n",
    "- 为每个块智能地创建了一组作业--在这种情况下，每个原始CSV文件都有一个作业。\n",
    "- 每个文件都被加载到一个pandas数据框中，并应用`len()`对其进行处理。\n",
    "- 将小计合并，得出最后的总数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 真实数据\n",
    "\n",
    "让我们用美国几年来的航班摘录来试试。这个数据是针对纽约市地区三个机场的航班的。\n",
    "\n",
    "上市公司财务报表数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意，数据框对象的respresentation不包含任何数据--Dask只是做了足够的工作来读取第一个文件的开始，并推断出列名和dtypes。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以查看数据的开始和结束。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "df.tail()  # this fails"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 发生了什么？\n",
    "\n",
    "与`pandas.read_csv`在推断数据类型之前读取整个文件不同，`dask.dataframe.read_csv`只读取文件开头的样本（如果使用glob，则读取第一个文件）。这些推断的数据类型会在读取所有分区时强制执行。\n",
    "\n",
    "在这种情况下，样本中推断的数据类型是不正确的。前`n`行没有`CRSElapsedTime`的值（pandas推断为`float`），后来变成了字符串（`object`dtype）。请注意，Dask会给出一个关于不匹配的错误信息。当这种情况发生时，你有几个选择。\n",
    "\n",
    "- 直接使用`dtype`关键字指定dtypes。这是推荐的解决方案，因为它是最不容易出错的（显式比隐式好），也是性能最好的。\n",
    "- 增加`sample`关键字的大小（以字节为单位）。\n",
    "- 使用 \"assume_missing \"使 \"dask \"假定推断为 \"int \"的列（不允许缺失值）实际上是floats（允许缺失值）。在我们的特殊情况下，这并不适用。\n",
    "\n",
    "在我们的例子中，我们将使用第一个选项，直接指定违规列的`dtypes`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dd.read_csv(os.path.join('data', 'nycflights', '*.csv'),\n",
    "                 parse_dates={'Date': [0, 1, 2]},\n",
    "                 dtype={'TailNum': str,\n",
    "                        'CRSElapsedTime': float,\n",
    "                        'Cancelled': bool})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()  # now works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用`dask.dataframe`进行计算\n",
    "\n",
    "我们计算`DepDelay`列的最大值。如果只用pandas，我们会在每个文件上循环找到各个最大值，然后在所有的最大值上找到最后的最大值。\n",
    "\n",
    "```python\n",
    "maxes = []\n",
    "for fn in filenames:\n",
    "    df = pd.read_csv(fn)\n",
    "    maxes.append(df.DepDelay.max())\n",
    "    \n",
    "final_max = max(maxes)\n",
    "```\n",
    "\n",
    "我们可以用`dask.delayed`来封装`d.read_csv`，这样它就可以并行运行。无论如何，我们还是要考虑循环、中间结果（每个文件一个）和最终的减少（中间最大值的`max`）。这只是真正的任务周围的噪音，pandas会用\n",
    "\n",
    "```python\n",
    "df = pd.read_csv(filename, dtype=dtype)\n",
    "df.DepDelay.max()\n",
    "```\n",
    "\n",
    "`dask.dataframe`让我们可以编写类似于pandas的代码，对大于内存的数据集进行并行操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time df.DepDelay.max().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这将为我们写入延迟计算，然后运行它。\n",
    "\n",
    "一些需要注意的事情。\n",
    "\n",
    "1.  和`dask.delayed`一样，我们需要在完成后调用`.compute()`。 在这之前，所有的东西都是懒惰的。\n",
    "2.  Dask会尽快删除中间结果（比如每个文件的完整pandas数据框架）。\n",
    "    - 这让我们可以处理比内存大的数据集。\n",
    "    - 这意味着重复计算每次都要把所有的数据加载进来（再运行上面的代码，是比你预期的快还是慢？\n",
    "\n",
    "与`Delayed`对象一样，你可以使用`.visualize`方法查看底层任务图。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the parallelism\n",
    "df.DepDelay.max().visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 练习\n",
    "\n",
    "在本节中，我们将进行一些`dask.dataframe`的计算。如果你对Pandas很熟悉，那么这些应该很熟悉。你将不得不考虑何时调用`compute`。\n",
    "\n",
    "### 1.) 我们的数据集中有多少条记录？\n",
    "\n",
    "如果你对pandas不熟悉，你会如何检查一个tuple的列表中有多少记录？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.) 总共乘坐了多少个未取消的航班？\n",
    "\n",
    "如果是pandas，你会使用[布尔索引](https://pandas.pydata.org/pandas-docs/stable/indexing.html#boolean-indexing)。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "len(df[~df.Cancelled])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.) 每个机场总共有多少个未取消的航班？\n",
    "\n",
    "*提示*: use [`df.groupby`](https://pandas.pydata.org/pandas-docs/stable/groupby.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df[~df.Cancelled].groupby('Origin').Origin.count().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.) 每个机场的平均起飞延误是多少？\n",
    "\n",
    "注意，这和你在上一个笔记本中的计算结果是一样的（这种方法是快了还是慢了？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(\"Origin\").DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.) 一周中哪一天的平均出发延误最严重？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "df.groupby(\"DayOfWeek\").DepDelay.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 分享中间成果\n",
    "\n",
    "在计算上述所有操作时，我们有时会不止一次地进行相同的操作。对于大多数操作，`dask.dataframe`会对参数进行哈希，允许重复的计算被共享，并且只计算一次。\n",
    "\n",
    "例如，让我们计算所有未取消航班的出发延误的平均值和标准差。由于dask操作是懒惰的，这些值还不是最终结果。它们只是得到结果所需的配方。\n",
    "\n",
    "如果我们用两次调用计算来计算它们，就不会出现中间计算的共享。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_cancelled = df[~df.Cancelled]\n",
    "mean_delay = non_cancelled.DepDelay.mean()\n",
    "std_delay = non_cancelled.DepDelay.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res = mean_delay.compute()\n",
    "std_delay_res = std_delay.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但让我们尝试将这两者传递给一个`compute`调用。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "mean_delay_res, std_delay_res = dask.compute(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`dask.compute`大约需要1/2的时间。这是因为在调用`dask.compute`时，两个结果的任务图都被合并，使得共享操作只做一次而不是两次。特别是，使用`dask.compute`只做一次以下操作。\n",
    "\n",
    "- 调用 \"read_csv \"和 \"dask.compute\"。\n",
    "- 过滤器(`df[~df.Cancelled]`)\n",
    "- 一些必要的还原(\"和\"、\"数\")\n",
    "\n",
    "要查看多个结果之间的合并任务图是什么样子的（以及共享的内容），可以使用`dask.visualize`函数（我们可能想使用`filename='graph.pdf'`将图保存到磁盘上，这样我们就可以更容易地放大）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.visualize(mean_delay, std_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 这与pandas相比，如何？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas比`dask.dataframe`更成熟，功能更齐全。 如果你的数据适合放在内存中，那么你应该使用Pandas。 当你对不适合在内存中操作的数据集进行操作时，`dask.dataframe`模块给你提供了有限的`pandas`体验。\n",
    "\n",
    "在本教程中，我们提供了一个由几个CSV文件组成的小数据集。 这个数据集在磁盘上是45MB，在内存中可扩展到约400MB。这个数据集足够小，你通常会使用Pandas。\n",
    "\n",
    "我们选择这个大小是为了让练习快速完成。 Dask.dataframe只有在比这个大得多的问题上才真正有意义，当Pandas用可怕的\n",
    "\n",
    "    MemoryError: ...\n",
    "\n",
    "此外，分布式调度器允许相同的数据框架表达式在一个集群中执行。为了实现大规模的 \"大数据 \"处理，可以执行数据摄取函数，比如`read_csv`，数据存放在每个worker节点都可以访问的存储上（比如amazon的S3），由于大部分操作只从选择一些列开始，对数据进行转换和过滤，所以机器之间只需要进行相对少量的数据通信。\n",
    "\n",
    "Dask.dataframe操作内部使用`pandas`操作。 一般来说，除了以下两种情况，它们的运行速度是差不多的。\n",
    "\n",
    "1.  Dask引入了一点开销，每个任务大约1ms。 这通常可以忽略不计。\n",
    "2.  当Pandas释放GIL时，`dask.dataframe`可以在一个进程内并行调用多个pandas操作，速度的提升与核心数成一定比例。对于不释放GIL的操作，需要多个进程才能获得同样的速度提升。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask DataFrame 数据模型\n",
    "\n",
    "在大多数情况下，Dask DataFrame感觉就像一个熊猫的DataFrame。\n",
    "到目前为止，我们所看到的最大的区别是Dask的操作是懒惰的；它们会建立一个任务图，而不是立即执行（更多细节将在[Schedulers](05_distributed.ipynb)中介绍）。\n",
    "这让Dask可以在核心之外并行地进行操作。\n",
    "\n",
    "在[Dask数组](03_array.ipynb)中，我们看到一个`dask.array`是由许多NumPy数组组成，沿着一个或多个维度分块。\n",
    "对于`dask.dataframe`来说也是如此：一个Dask DataFrame是由许多pandas DataFrames组成的。对于`dask.dataframe`来说，分块只沿着索引发生。\n",
    "\n",
    "<img src=\"http://dask.pydata.org/en/latest/_images/dask-dataframe.svg\" width=\"30%\">\n",
    "\n",
    "我们把每个分块称为*分区*，上/下界是*分部*。\n",
    "Dask *可以*存储关于分区的信息。现在，当你写自定义函数应用于Dask DataFrames时，分区就会出现。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将 \"CRSDepTime \"转换为时间戳\n",
    "\n",
    "该数据集存储的时间戳为`HHMM`，在`read_csv`中作为整数读入。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crs_dep_time = df.CRSDepTime.head(10)\n",
    "crs_dep_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了将这些转换为预定出发时间的时间戳，我们需要将这些整数转换为`pd.Timedelta`对象，然后将它们与`Date`列结合起来。\n",
    "\n",
    "在pandas中，我们会使用`pd.to_timedelta`函数，并进行一些运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Get the first 10 dates to complement our `crs_dep_time`\n",
    "date = df.Date.head(10)\n",
    "\n",
    "# Get hours as an integer, convert to a timedelta\n",
    "hours = crs_dep_time // 100\n",
    "hours_timedelta = pd.to_timedelta(hours, unit='h')\n",
    "\n",
    "# Get minutes as an integer, convert to a timedelta\n",
    "minutes = crs_dep_time % 100\n",
    "minutes_timedelta = pd.to_timedelta(minutes, unit='m')\n",
    "\n",
    "# Apply the timedeltas to offset the dates by the departure time\n",
    "departure_timestamp = date + hours_timedelta + minutes_timedelta\n",
    "departure_timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 自定义代码和Dask数据框架\n",
    "\n",
    "我们可以将 \"pd.to_timedelta \"换成 \"dd.to_timedelta\"，并在整个dask DataFrame上做同样的操作。但是，假设Dask没有实现`dd.to_timedelta`在Dask DataFrames上工作。那么你会怎么做呢？\n",
    "\n",
    "`dask.dataframe`提供了一些方法来使应用自定义函数到Dask DataFrames更容易。\n",
    "\n",
    "- [`map_partitions`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_partitions)\n",
    "- [`map_overlap`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.map_overlap)\n",
    "- [`reduction`](http://dask.pydata.org/en/latest/dataframe-api.html#dask.dataframe.DataFrame.reduction)\n",
    "\n",
    "这里我们只讨论`map_partitions`，我们可以用它来自己实现`to_timedelta`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the docs for `map_partitions`\n",
    "\n",
    "help(df.CRSDepTime.map_partitions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "基本的想法是将一个对DataFrame进行操作的函数应用到每个分区。\n",
    "在本例中，我们将应用`pd.to_timedelta`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hours = df.CRSDepTime // 100\n",
    "# hours_timedelta = pd.to_timedelta(hours, unit='h')\n",
    "hours_timedelta = hours.map_partitions(pd.to_timedelta, unit='h')\n",
    "\n",
    "minutes = df.CRSDepTime % 100\n",
    "# minutes_timedelta = pd.to_timedelta(minutes, unit='m')\n",
    "minutes_timedelta = minutes.map_partitions(pd.to_timedelta, unit='m')\n",
    "\n",
    "departure_timestamp = df.Date + hours_timedelta + minutes_timedelta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departure_timestamp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "departure_timestamp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习：重写上面的内容，只需调用 \"map_partitions\n",
    "\n",
    "这将比两次单独调用的效率略高，因为它减少了图中的任务数量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_departure_timestamp(df):\n",
    "    pass  # TODO: implement this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "raises-exception"
    ]
   },
   "outputs": [],
   "source": [
    "departure_timestamp = df.map_partitions(compute_departure_timestamp)\n",
    "\n",
    "departure_timestamp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def compute_departure_timestamp(df):\n",
    "    hours = df.CRSDepTime // 100\n",
    "    hours_timedelta = pd.to_timedelta(hours, unit='h')\n",
    "\n",
    "    minutes = df.CRSDepTime % 100\n",
    "    minutes_timedelta = pd.to_timedelta(minutes, unit='m')\n",
    "\n",
    "    return df.Date + hours_timedelta + minutes_timedelta\n",
    "\n",
    "departure_timestamp = df.map_partitions(compute_departure_timestamp)\n",
    "departure_timestamp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 限制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 哪些地方不能用？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dask.dataframe只涵盖了Pandas API的一小部分，但使用得很好。\n",
    "这种限制有两个原因。\n",
    "\n",
    "1.  Pandas API是*大的\n",
    "2.  有些操作确实很难并行完成（如排序）。\n",
    "\n",
    "此外，一些重要的操作，如``set_index``可以工作，但比Pandas慢，因为它们包括大量的数据洗牌，可能会写到磁盘上。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 了解更多\n",
    "\n",
    "\n",
    "* [DataFrame documentation](https://docs.dask.org/en/latest/dataframe.html)\n",
    "* [DataFrame screencast](https://youtu.be/AT2XtFehFSQ)\n",
    "* [DataFrame API](https://docs.dask.org/en/latest/dataframe-api.html)\n",
    "* [DataFrame examples](https://examples.dask.org/dataframe.html)\n",
    "* [Pandas documentation](https://pandas.pydata.org/pandas-docs/stable/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.shutdown()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}